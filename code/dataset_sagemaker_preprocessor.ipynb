{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def abc_arr_creater():\n",
    "    letter = 'a'\n",
    "    arr = list()\n",
    "    count = 1\n",
    "    while (count < 27):\n",
    "        if letter != 'w':\n",
    "            arr.append(letter)\n",
    "        letter = chr(ord(letter) + 1)\n",
    "        count += 1\n",
    "    return arr\n",
    "\n",
    "def create_dataset_folders():\n",
    "    dataset_arr = ['train', 'test', 'validation']\n",
    "    alphabet_arr = abc_arr_creater()\n",
    "    diractory = 'lips_cut_dataset'\n",
    "    parent_dir = ''\n",
    "    path = os.path.join(parent_dir, diractory)\n",
    "    for set_dir in dataset_arr:\n",
    "        set_path = os.path.join(path, set_dir)\n",
    "        for letter in alphabet_arr:\n",
    "            letter_dir_path = os.path.join(set_path, letter)\n",
    "            os.makedirs(letter_dir_path)\n",
    "            print('directory ' + letter + ' in ' + set_dir + ' created')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory a in train created\n",
      "directory b in train created\n",
      "directory c in train created\n",
      "directory d in train created\n",
      "directory e in train created\n",
      "directory f in train created\n",
      "directory g in train created\n",
      "directory h in train created\n",
      "directory i in train created\n",
      "directory j in train created\n",
      "directory k in train created\n",
      "directory l in train created\n",
      "directory m in train created\n",
      "directory n in train created\n",
      "directory o in train created\n",
      "directory p in train created\n",
      "directory q in train created\n",
      "directory r in train created\n",
      "directory s in train created\n",
      "directory t in train created\n",
      "directory u in train created\n",
      "directory v in train created\n",
      "directory x in train created\n",
      "directory y in train created\n",
      "directory z in train created\n",
      "directory a in test created\n",
      "directory b in test created\n",
      "directory c in test created\n",
      "directory d in test created\n",
      "directory e in test created\n",
      "directory f in test created\n",
      "directory g in test created\n",
      "directory h in test created\n",
      "directory i in test created\n",
      "directory j in test created\n",
      "directory k in test created\n",
      "directory l in test created\n",
      "directory m in test created\n",
      "directory n in test created\n",
      "directory o in test created\n",
      "directory p in test created\n",
      "directory q in test created\n",
      "directory r in test created\n",
      "directory s in test created\n",
      "directory t in test created\n",
      "directory u in test created\n",
      "directory v in test created\n",
      "directory x in test created\n",
      "directory y in test created\n",
      "directory z in test created\n",
      "directory a in validation created\n",
      "directory b in validation created\n",
      "directory c in validation created\n",
      "directory d in validation created\n",
      "directory e in validation created\n",
      "directory f in validation created\n",
      "directory g in validation created\n",
      "directory h in validation created\n",
      "directory i in validation created\n",
      "directory j in validation created\n",
      "directory k in validation created\n",
      "directory l in validation created\n",
      "directory m in validation created\n",
      "directory n in validation created\n",
      "directory o in validation created\n",
      "directory p in validation created\n",
      "directory q in validation created\n",
      "directory r in validation created\n",
      "directory s in validation created\n",
      "directory t in validation created\n",
      "directory u in validation created\n",
      "directory v in validation created\n",
      "directory x in validation created\n",
      "directory y in validation created\n",
      "directory z in validation created\n"
     ]
    }
   ],
   "source": [
    "create_dataset_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "\n",
    "def get_video_files():  # get the names of the video files to a list\n",
    "    video_files = list()\n",
    "    path_to_video = 'front'  # url to dir where the dataset video locate\n",
    "    for file in os.listdir(path_to_video):\n",
    "        if file.endswith('mov'):  # if file end with mov add it to the list\n",
    "            video_files.append(file)\n",
    "    return video_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_files():  # get the jason files to a list\n",
    "    jason_files = list()\n",
    "    path_to_jason_files = 'alignment'  # url path for the text of the dataset\n",
    "    for pos_json in os.listdir(path_to_jason_files):  # loop at the dir and look for jason flies\n",
    "        if pos_json.endswith('.json'):\n",
    "            jason_files.append(pos_json)\n",
    "    return jason_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_videos(jsonFiles, temp_videoNames):\n",
    "    valid_videos = []\n",
    "    for file in temp_videoNames:\n",
    "        if file in jsonFiles:\n",
    "            valid_videos.append(file + '.mov')\n",
    "    return valid_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_text(jason_list, videofile):\n",
    "    video_name = videofile.replace('.mov', '.json')  # name of the jason file\n",
    "    jason_data = ''\n",
    "    url = 'alignment'  # location of the file\n",
    "    for file in jason_list:\n",
    "        if file == video_name:  # if file name as the name of the video then open it as jason file\n",
    "            with open('alignment/' + video_name) as json_file:\n",
    "                jason_data = json.load(json_file)  # jason_data is now a jason file\n",
    "                break\n",
    "    return jason_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLetterTime(data, video_name): # find the offset and the end offset of the letter\n",
    "    counter = 0\n",
    "    start_time = ''\n",
    "    end_time = ''\n",
    "    for p in data[video_name]:\n",
    "        if '_B' in str(p['phone']):\n",
    "            counter += 1\n",
    "        if counter == 4:\n",
    "            start_time = p['offset']\n",
    "            counter += 1\n",
    "        if counter > 4 and '_E' in str(p['phone']):\n",
    "            end_time = p['offset'] + p['duration']\n",
    "            break\n",
    "        if '_S' in str(p['phone']) and counter >= 3:\n",
    "            start_time = p['offset']\n",
    "            end_time = p['offset'] + p['duration']\n",
    "            break\n",
    "    return start_time,end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAndSaveFrames(cap, path, videoName, dircount):\n",
    "    count = 0\n",
    "    frame_counter = 0\n",
    "    while (cap.isOpened()):\n",
    "        ret, frame = cap.read()  # read the frame\n",
    "        if ret == True and frame_counter <= 14:\n",
    "            cv2.imwrite(path + '/frame%d.jpg' % count, frame)  # save the current frame as framenumber.jpg\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n",
    "        frame_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procces(json_files, validVideofiles): # run the videos and divide them to test, train, validation \n",
    "    dataset_arr = ['train','test', 'validation']\n",
    "    dircount = 0\n",
    "    for setdir in dataset_arr:\n",
    "        if setdir == 'train':\n",
    "            limit = 4000 # in the train there will be frames from the first 4000 videos \n",
    "        elif setdir == 'test':\n",
    "            limit = 5000\n",
    "        else:\n",
    "            limit = 5319\n",
    "        while dircount < limit:\n",
    "            file = validVideofiles[dircount]\n",
    "            data = get_video_text(json_files, file)  # get the video text\n",
    "            videoName = file.split('.')[0]\n",
    "            letterStart, letterEnd = findLetterTime(data, videoName)  # get the letter start time\n",
    "            letter = str(dircount) + videoName.split('_')[2][3]  # letter and counter\n",
    "            letter_class = videoName.split('_')[2][3]  # the letter class in the set\n",
    "            cap = cv2.VideoCapture('front/' + file)  # get the video file\n",
    "            constant = 222.2222223  # number of melsec/100 to add to get the requested 3 frames\n",
    "            os.mkdir('updated_dataset/' + setdir + '/' + letter_class + '/' + letter) # make the dir number_letter in the set\n",
    "            \n",
    "            # the loop was here\n",
    "            offset_mel = letterStart * 1000\n",
    "            duration_mel = (letterEnd - letterStart) * 1000 + constant\n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC, offset_mel) # run the video from the letter start point\n",
    "            path = 'updated_dataset/' + setdir + '/' + letter_class + '/' + letter # the path to save the frames\n",
    "            runAndSaveFrames(cap, path, videoName, dircount)\n",
    "        #  Release everything if job is finished\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print('directory ' + letter + ' saved in ' + path )\n",
    "            dircount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    tempVideo_files = get_video_files()\n",
    "    json_files = get_json_files()  # get all the texts\n",
    "    jsonFilesNames = []\n",
    "    temp_videoNames = []\n",
    "    for jsonfile in json_files:\n",
    "        jsonNamesSplit = jsonfile.split('.')\n",
    "        jsonFilesNames.append(jsonNamesSplit[0])\n",
    "    for video in tempVideo_files:\n",
    "        videoSplit = video.split('.')\n",
    "        temp_videoNames.append(videoSplit[0])\n",
    "\n",
    "    validVideofiles = get_valid_videos(jsonFilesNames, temp_videoNames)\n",
    "    procces(json_files, validVideofiles)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s52_p_srwe5s.mov\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
